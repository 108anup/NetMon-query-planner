* Tasks

** When possible
*** TODO 
    [2020-04-28 Tue]
    [[file:~/Netmon/NetMon-query-planner/gurobi/devices.py::fraction_parallel%20=%203/4]]
*** TODO 
    [2020-04-28 Tue]
    [[file:~/Netmon/NetMon-query-planner/gurobi/devices.py::m.addGenConstrPWL(mem,%20md.m_access_time,%20self.mem_par,%20self.mem_ns,]]
*** TODO Consider memoizing solutions to clusters?
*** TODO group by device objectives and constrains with priorities
    [2020-04-29 Wed]
    [[file:~/Netmon/NetMon-query-planner/gurobi/solvers.py::class%20UnivmonGreedy(Univmon):]]

** Soon
*** TODO extra asserts
    [2020-04-28 Tue]
    [[file:~/Netmon/NetMon-query-planner/gurobi/devices.py::assert(m%20is%20None)%20#%20TODO:%20can%20remove%20later]]
*** TODO refine cache
    [2020-04-28 Tue]
    [[file:~/Netmon/NetMon-query-planner/gurobi/solvers.py::cache%20=%20{}%20#%20TODO:%20see%20if%20there%20is%20more%20performant%20cache]]
*** TODO impact of int
    [2020-04-28 Tue]
    [[file:~/Netmon/NetMon-query-planner/gurobi/solvers.py::key%20=%20(d.config_name,%20int(md.mem_tot),%20int(md.rows_tot))]]
*** TODO [#A] urgent
    DEADLINE: <2020-04-30 Thu 14:00-17:00>
    [2020-04-29 Wed]
    [[file:~/Netmon/NetMon-query-planner/gurobi/devices.py::#%20TODO::%20Can%20memoize,%20See%20pickle]]
*** TODO Need to change everything that reads 
    inp.devices and also check solvers.py
    <2020-04-29 Wed>
*** TODO [#A] Urgent
    <2020-04-29 Wed>
    [[file:~/Netmon/NetMon-query-planner/gurobi/solvers.py::#%20TODO:%20If%20these%20are%20not%20vars%20then%20no%20need%20of]]
*** TODO Consider having clusters also without chunks





bibliography:/mnt/E/YandexDisk/CMU/Research/reading-material/references.bib

* DISCUSS Things that must be done
  DEADLINE: <2020-05-17 Sun>
  :PROPERTIES:
  :COLUMNS:  %TODO %70ITEM(Task) %TIME(Time){:} %DEADLINE(Deadline)
  :PROPERTY: time
  :END:
** WRITING better name for system
**** SPARE (Scalable, Performant, Accurate and Resource Efficient)
**** NetMon
** fix and incorporate netronome NIC with solver
   DEADLINE: <2020-05-05 Tue>
   :PROPERTIES:
   :TIME:     0.5d
   :END:
*** DISCUSS the model
*** EXPERIMENT verify non linear hashing cost, why is this coming in the first place
*** EXPERIMENT netronome resources
** Univmon / UGR without clustering vs Netmon with clustering
   DEADLINE: <2020-05-05 Tue>
*** EXPERIMENT Find the experiment when Netmon with clustering was better than Univmon without clustering 
*** CODE parallelize solver
    :PROPERTIES:
    :TIME:     1d
    :END:
*** CODE Intelligently decide the cluster sizes
    :PROPERTIES:
    :TIME:     0.5d
    :END:
** CODE Traffic incorporation when modeling performance
   :PROPERTIES:
   :TIME:     1d
   :END:
** memoization
   DEADLINE: <2020-05-07 Thu>
   :PROPERTIES:
   :TIME:     0.5d
   :END:
*** CODE make memoization key more precise
    :PROPERTIES:
    :TIME:
    :END:
*** CODE need to change after traffic incorporation
    :PROPERTIES:
    :TIME:
    :END:
** CODE test bed setup
   DEADLINE: <2020-05-08 Fri>
   :PROPERTIES:
   :TIME:     1d
   :END:
*** topology generator
**** DONE Tree
**** Clos
**** Internet2
**** TopologyZoo
*** traffic generator
*** sketch requirement generator
** OpenVSwitch style and multicore implementation
   DEADLINE: <2020-05-10 Sun>
   :PROPERTIES:
   :TIME:     1d 0:00
   :END:
*** EXPERIMENT profiling and verification
    :PROPERTIES:
    :TIME:     0.5d
    :END:
*** CODE multicore implementation
    :PROPERTIES:
    :TIME:     0.5d
    :END:
** model based
   DEADLINE: <2020-05-14 Thu>
*** dynamics 
    DEADLINE: <2020-05-13 Wed>
    :PROPERTIES:
    :TIME:     3d 0:00
    :END:
**** CODE warm start
     :PROPERTIES:
     :TIME:     1d
     :END:
**** EXPERIMENT warm start
     :PROPERTIES:
     :TIME:     2d
     :END:
*** EXPERIMENT Generate graphs solving time
    DEADLINE: <2020-05-14 Thu>
    :PROPERTIES:
    :TIME:     0.5d
    :END:
**** trends
     :PROPERTIES:
     :TIME:
     :END:
**** comparison with prior (with and without clustering)
     :PROPERTIES:
     :TIME:
     :END:
*** EXPERIMENT Generate graphs optimality
    :PROPERTIES:
    :TIME:     0.5d
    :END:
** Evaluation - one device at a time 
   DEADLINE: <2020-05-17 Sun>
   :PROPERTIES:
   :TIME:     3d 0:00
   :END:
*** CODE setup - convert solver output to running script and profiling task
    :PROPERTIES:
    :TIME:     2d
    :END:
*** EXPERIMENT running
    :PROPERTIES:
    :TIME:     1d
    :END:
** EXPERIMENT prototype evaluation (can't do without physical conn changes, can do some?)
   :PROPERTIES:
   :TIME:
   :END:
** CODE add more types of sketches
   :PROPERTIES:
   :TIME:     2d
   :END:
**** Count Sketch
**** Univmon
**** HyperLogLog
**** Hierarchical Heavy Hitters?
*** EXPERIMENT Non linear accuracy relations
    :PROPERTIES:
    :TIME:     1d
    :END:
** TODO need to email barefoot Faster with paper 10 days before for checking IP violations
** CODE Spectral clustering time (general case)
   :PROPERTIES:
   :TIME:     1d
   :END:
   Based on [[https://www.ijcai.org/Proceedings/13/Papers/222.pdf][large scale spectral clustering]] for 100k nodes, spectral clustering should take a couple of seconds. 
   
   The scipy implementation seems to much more time (more than minutes) did not run completely.
   Based on [[https://hdbscan.readthedocs.io/en/latest/performance_and_scalability.html][clustering-bench]] sklearn spectral is slow (interactive only for 5000 nodes)
   Another alternative could be [[https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html][hdbscan]], which is very fast
   Need to see if it can give us good clusters that we need


* PAPER Our assumptions
** Heap overheads
*** HHs are reported using separate packets at a low enough frequency that  has negligible performance overhead.
*** Q/A
    :q: What about static overhead of reporting HH in P4, calculating min?
    :a: No need to calculate min, if at any point value exceeds threshold, just report. The controller will take care of min operations.
    The memory can be polled using control plane apps (netro, tofino) and using shared memory regions in CPU    


* PAPER discussion (subtleties)
** DISCUSS ? sometimes Netmon can take more resources
   This happens when different clusters have different ns and Netmon places to optimize for that ns
   This problem does not arise if we have a traffic requirement
** restricting cols to power of 2 alleviates need to constrain packing in P4, along with being feasible


* PAPER Design details
** DISCUSS multi core CPU sketch 
   1. different rows on different 
   2. packet spraying based on some key in header
      + This will cause extra L1 / L2 memory.
      + For non linear accuracy models, this will require more memory


* Background and Motivation
** WRITING Uses of monitoring (can look for more relevant examples)
   - Security (VM compromise detection cite:private-eye)
   - Resource Provisioning (cite:traffic-demands-application)
   - Billing (cite:accounting-application)
** Expectations from network operators
*** Performance 
**** cite:private-eye
     At each end host, for every 10s:
     Flow sample and keep data for 5000 flows at a time, a sketch can do better by providing a better mem-accuracy trade-off
      
     It is relevant for us as:
     + Queries can be captured using HH
       + Bytes sent to IP a.b.c.d over time by each VM
       + Flow size distribution for each VM
     They use CDFs which they bucket into top 1%, 10% etc. => HH style query
*** Resources
    cite:vcrib "RackSpace operators prefer not to dedicate even a portion of a server core for rule processing [6]"
    cite:microsoft-fpga cores are money
    cite:vcrib TCAM power hungry
*** Accuracy
    Obvious?
    cite:private-eye carefully chooses 5000 based on fraction of VMs which have more than 5000 flows.
*** Network-wide (need both end-hosts and in-network devices)
    cite:private-eye NetFlow/IPFix do not capture flows that do not traverse the network core.
    cite:pathdump packets may not reach the destination etc. (spurious drops)
    - can use HH to count packets dropped per flow / origin as well
    With NetCache like works, packets again may not reach servers
*** Predictability and reliability
    cite:microsoft-fpga cloud providers and network operators don't like variability in performance
*** Choose right sampling method according to situation: Flow-sampling
    cite:private-eye NetFlow/IPFix systems are used for traffic engineering,
    DDOS protection, and other tasks. They run on core
    routers and sample 1 out of 4096 packets traversing the network core. 
    Biased towards heavy flows
*** There is a benefit of a central monitoring requirement store
    If left to the will of tenants in cloud settings, multiple tenants can
** Devices are changing
*** Memory is not a proxy for cost
    If we only consider memory then we will put a lot of load on CPU (high capacity)
    This will lead to either high resource usage (CPU cores)
    or lead to poor performance (low throughput)
    => Need to consider compute resources / performance
*** Some devices have discrete resources like cores (polled)
    This introduces a concept of fitting, (flat cost)
    Can show cost vs sketch manifest graph (step wise)
*** Are switches underutilized in some locations in the network?
    Better packing
*** Flexibility - other works talk about
    - difference in flexibility
      - reassembly
      - complex control flow
    We can say that for queries requiring reassembly type operations we need to incorporate CPUs
**** TODO Are there sketches which can only be implemented on one type of device?
** Current solutions are falling short of addressing trends in modern networks
   cite:private-eye can do much better (more accuracy, lower performance/resource overhead)
   cite:vcrib rule / sampling based -> can use sketches for lower resource usage (hence better performance)
   cite:univmon memory as proxy for load -> will lead to high perf overhead for CPUs
   if not leveraging step wise then loosing out on benefits
    

* Micro optimizations in code
** CODE keep a single bench profile rather than copying the variables for each device.
   This might change when we add device load as well
   

* Meeting updates
** <2020-05-04 Mon>
   - Doing vertical partitioning only on CPU seems to give same benefits
   - Discuss netronome model
   - Discuss timeline and outline
   - emulab setup
   - Why is overlay=none taking lesser time.
   :q: Why is selective refinement degrading solution. 
   :a: due to wrong caching, don't take ceiling or floor of rows -> sensitive param!

   
* Emulab setup
  Got 4x Mellanox NICs (16, 17, 18, 19)
  Got 2x Intel NICs (20, 22)
  Got 2x Netronome NICs (12, 13)
  Got 2x Tofino switches (T1, T2)

  Propsed changes:
  beluga12:netro0 - tofino1:7
  beluga13:netro0 - tofino2:7

  beluga20:fge0 - tofino1:8
  beluga21:fge0 - tofino1:9
  beluga20:fge1 - tofino2:3
  beluga21:fge1 - tofino2:4

  beluga16:fge1 - tofino2:5
  beluga17:fge1 - tofino2:6

  Original:
  tofino1:1 - beluga14:fge0
  tofino1:2 - beluga15:fge0
  tofino1:3 - beluga16:fge0
  tofino1:4 - beluga17:fge0
  tofino1:5 - beluga18:fge0
  tofino1:6 - beluga19:fge0
  tofino1:32 - tofino2:32
  beluga14:fge1 - beluga15:fge1
  beluga16:fge1 - beluga17:fge1
  beluga18:fge1 - tofino2:1
  beluga19:fge1 - tofino2:2
  beluga20:fge0 - beluga21:fge0
  beluga22:fge0 - beluga3:fge1
  beluga22:fge1 - beluga4:fge1
  beluga1:fge0 - beluga2:fge0
  beluga12:fge1 - beluga13:fge1
  beluga12:netro0 - beluga13:netro0
